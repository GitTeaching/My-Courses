{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8vQKUh-pjdJ"
   },
   "source": [
    "## **TP 3 - Analyse Lexicale et PreProcessing avec NLTK.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5JTPS0qrPZZ"
   },
   "source": [
    "### **Installer et Importer les librairies modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09Egu-FpMTD0",
    "outputId": "a66bf792-d5a9-42fd-e7b0-5279c480271f"
   },
   "outputs": [],
   "source": [
    "# importing tokenization \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# importing the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# importing Porter and Lancaster stemmers from nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "# importing WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# importing wordnet\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a text written.', 'It uses U.S. english to illustrate sentence tokenization.']\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a text written. It uses U.S. english to illustrate sentence tokenization.'\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ce texte est écrit.', \"Il a comme but d'illustrer la segmentation d'un texte en français.\"]\n"
     ]
    }
   ],
   "source": [
    "fr_text = \"Ce texte est écrit. Il a comme but d'illustrer la segmentation d'un texte en français.\"\n",
    "fr_sents = sent_tokenize(fr_text, language='french')\n",
    "\n",
    "print(fr_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization - different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'text', 'written', '.', 'It', 'uses', 'U.S.', 'english', 'to', 'illustrate', 'word', \"'s\", 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a text written. It uses U.S. english to illustrate word\\'s tokenization.'\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ce', 'texte', 'est', 'écrit', '.', 'Il', 'a', 'comme', 'but', \"d'illustrer\", 'la', 'segmentation', \"d'un\", 'texte', 'en', 'français', '.']\n"
     ]
    }
   ],
   "source": [
    "fr_text = \"Ce texte est écrit. Il a comme but d'illustrer la segmentation d'un texte en français.\"\n",
    "fr_words = word_tokenize(fr_text, language='french')\n",
    "\n",
    "print(fr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "# The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. \n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'''\n",
    "tokens = TreebankWordTokenizer().tokenize(s)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(s)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "tokens = regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "s = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tokens = tknzr.tokenize(s)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGOdsCIqp4K9"
   },
   "source": [
    "### Stop words suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'bengali',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esw = stopwords.words('english')\n",
    "\n",
    "esw[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsw = stopwords.words('french')\n",
    "\n",
    "fsw[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'New', 'York', '.', 'Please', 'buy', 'two', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "words = ['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', \n",
    "         'Please', 'buy', 'me', 'two', 'of', 'them',  '.', 'Thanks', '.']\n",
    "\n",
    "filtered = [w for w in words if not w.lower() in esw]\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGOdsCIqp4K9"
   },
   "source": [
    "### **Stemming** - Porter and Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVmHgw4GMb7G",
    "outputId": "e44459cf-c764-40f4-9b8d-c05a162ec29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "probabl\n",
      "\n",
      "- Lancaster Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "prob\n"
     ]
    }
   ],
   "source": [
    "# LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Aggressive stemming.\n",
    "# Over-stemming causes the stems to be not linguistic, or they may have no meaning. \n",
    "# Lancaster produces an even shorter stem than Porter because of iterations and over-stemming is occurred.\n",
    "\n",
    "# create an object of class PorterStemmer and LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# - Stemming a word - PorterStemmer\n",
    "print(\"- Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(porter.stem(\"probably\"))\n",
    "\n",
    "print()\n",
    "\n",
    "# - Stemming a word - LancasterStemmer \n",
    "print(\"- Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))\n",
    "print(lancaster.stem(\"probably\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArabicStemmer',\n",
       " 'DanishStemmer',\n",
       " 'DutchStemmer',\n",
       " 'EnglishStemmer',\n",
       " 'FinnishStemmer',\n",
       " 'FrenchStemmer',\n",
       " 'GermanStemmer',\n",
       " 'HungarianStemmer',\n",
       " 'ItalianStemmer',\n",
       " 'NorwegianStemmer',\n",
       " 'PorterStemmer',\n",
       " 'PortugueseStemmer',\n",
       " 'RomanianStemmer',\n",
       " 'RussianStemmer',\n",
       " 'SnowballStemmer',\n",
       " 'SpanishStemmer',\n",
       " 'StemmerI',\n",
       " 'SwedishStemmer',\n",
       " '_LanguageSpecificStemmer',\n",
       " '_ScandinavianStemmer',\n",
       " '_StandardStemmer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'demo',\n",
       " 'porter',\n",
       " 'prefix_replace',\n",
       " 're',\n",
       " 'stopwords',\n",
       " 'suffix_replace']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import snowball\n",
    "dir(snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64YHov0jMkST",
    "outputId": "654dabb0-2353-4bd1-fa82-7a81b6982919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Lancaster Stemmer\n",
      "friend              friend              friend\n",
      "friendship          friendship          friend\n",
      "friends             friend              friend\n",
      "friendships         friendship          friend\n",
      "stabil              stabil              stabl\n",
      "destabilize         destabil            dest\n",
      "misunderstanding    misunderstand       misunderstand\n",
      "railroad            railroad            railroad\n",
      "moonlight           moonlight           moonlight\n",
      "football            footbal             footbal\n"
     ]
    }
   ],
   "source": [
    "# - Stemming a list of words\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20}{1:20}{2}\".format(\"Word\", \"Porter Stemmer\", \"Lancaster Stemmer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2}\".format(word, porter.stem(word), lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Pythoners', 'are', 'very', 'intelligent', ',', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
      "Stems:  ['python', 'are', 'veri', 'intellig', ',', 'and', 'work', 'veri', 'pythonli', 'and', 'now', 'they', 'are', 'python', 'their', 'way', 'to', 'success', '.']\n"
     ]
    }
   ],
   "source": [
    "# - Stemming a sentence with word tokenization (punctuations are keeped with word_tokenizer)\n",
    "\n",
    "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "# Tokenization\n",
    "token_words = word_tokenize(sentence)\n",
    "print('Tokens:', token_words)\n",
    "\n",
    "# Stemming\n",
    "stem_sentence = []\n",
    "for word in token_words:\n",
    "    stem_sentence.append(porter.stem(word))\n",
    "    \n",
    "print('Stems: ', stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens - before: ['Pythoners', 'are', 'very', 'intelligent', ',', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
      "Tokens - after: ['Pythoners', 'intelligent', ',', 'work', 'pythonly', 'pythoning', 'way', 'success', '.']\n",
      "Stems:  ['python', 'intellig', ',', 'work', 'pythonli', 'python', 'way', 'success', '.']\n"
     ]
    }
   ],
   "source": [
    "# - Stemming a sentence with tokenization, without stopwords\n",
    "\n",
    "# downloading stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# assigning the english stop-words to the sw list\n",
    "esw = stopwords.words('english')\n",
    "\n",
    "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "# Tokenization\n",
    "token_words = word_tokenize(sentence)\n",
    "print('Tokens - before:', token_words)\n",
    "\n",
    "# Eliminate the stop words from the tokens\n",
    "clean_tokens = [token for token in token_words if token not in esw]\n",
    "print('Tokens - after:', clean_tokens)\n",
    "\n",
    "# Stemming\n",
    "stem_sentence = []\n",
    "for word in clean_tokens:\n",
    "    stem_sentence.append(porter.stem(word))\n",
    "    \n",
    "print('Stems: ', stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens - before: ['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success']\n",
      "Tokens - after: ['Pythoners', 'intelligent', 'work', 'pythonly', 'pythoning', 'way', 'success']\n",
      "Stems:  ['python', 'intellig', 'work', 'pythonli', 'python', 'way', 'success']\n"
     ]
    }
   ],
   "source": [
    "# - Stemming a sentence with word tokenization (remove punctuations with RegexpTokenizer)\n",
    "\n",
    "# downloading stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# assigning the english stop-words to the sw list\n",
    "esw = stopwords.words('english')\n",
    "\n",
    "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_words = tokenizer.tokenize(sentence)\n",
    "print('Tokens - before:', token_words)\n",
    "\n",
    "# Eliminate the stop words from the tokens\n",
    "clean_tokens = [token for token in token_words if token not in esw]\n",
    "print('Tokens - after:', clean_tokens)\n",
    "\n",
    "# Stemming\n",
    "stem_sentence = []\n",
    "for word in clean_tokens:\n",
    "    stem_sentence.append(porter.stem(word))\n",
    "    \n",
    "print('Stems: ', stem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUaLlSPoq_2W"
   },
   "source": [
    "### **Lemmatization**  - WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQQYLwtlRaEj",
    "outputId": "6cdceafb-3056-44f5-f8fc-935ce51a8e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "foot\n",
      "are\n",
      "be\n",
      "swim\n",
      "swimming\n",
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "# - Lemmatization - WordNet Lemmatizer without context\n",
    "\n",
    "# download wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# instantiating the lemmaztizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize a single word without context\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "\n",
    "# Lemmatize a single word with context\n",
    "print(lemmatizer.lemmatize(\"are\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"swimming\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"swimming\", pos='n'))\n",
    "print(lemmatizer.lemmatize(\"stripes\", pos='v')) \n",
    "print(lemmatizer.lemmatize(\"stripes\", pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wolv', 'wolf')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diffrence between Stemming and Lemmatization\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = 'wolves'\n",
    "\n",
    "stem = stemmer.stem(word)\n",
    "lemma = lemmatizer.lemmatize(word)\n",
    "\n",
    "stem, lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YILtwedsr0uu",
    "outputId": "a305d756-deb0-40c5-caf5-3b184522b92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma\n",
      "He                  He\n",
      "was                 wa\n",
      "running             running\n",
      "and                 and\n",
      "eating              eating\n",
      "at                  at\n",
      "same                same\n",
      "time                time\n",
      "He                  He\n",
      "has                 ha\n",
      "bad                 bad\n",
      "habit               habit\n",
      "of                  of\n",
      "swimming            swimming\n",
      "after               after\n",
      "playing             playing\n",
      "long                long\n",
      "hours               hour\n",
      "in                  in\n",
      "the                 the\n",
      "Sun                 Sun\n"
     ]
    }
   ],
   "source": [
    "# - Lemmatize a sentence\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# tokenize the sentence into a list of words without punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentence_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "# without context\n",
    "print(\"{0:20}{1}\".format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1}\".format(word, lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance - Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wordnet.synsets('dog')[0]\n",
    "\n",
    "dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cat.n.01')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = wordnet.synsets('cat')[0]\n",
    "\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wu-Palmer Similarity\n",
    "dog.wup_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Série_TP_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
