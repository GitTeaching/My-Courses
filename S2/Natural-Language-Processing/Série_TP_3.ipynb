{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8vQKUh-pjdJ"
   },
   "source": [
    "# Série TP 3 - TALN - Traitements Lexicaux - avec NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - POS Tagging, Stemming, et Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'est-ce que le package Python **NLTK** ?\n",
    "\n",
    "Natural Language Tool Kit (NLTK) est une bibliothèque Python permettant de créer des programmes fonctionnant avec le langage naturel. Il fournit une interface conviviale aux ensembles de données contenant plus de 50 corpus et ressources lexicales telles que WordNet. La bibliothèque peut effectuer différentes opérations telles que la tokenization, le stemming, la lemmatisation, la classification, le parsing, le pos tagging, etc.\n",
    "\n",
    "NLTK peut être utilisé par les étudiants, les chercheurs et les industriels. C'est une bibliothèque Open Source et gratuite. Il est disponible pour Windows, Mac OS et Linux.\n",
    "\n",
    "- Pour l’installer en utilisant PIP depuis cmd.exe : >> pip install nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation et Téléchargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\LeE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LeE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LeE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LeE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\LeE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5JTPS0qrPZZ"
   },
   "source": [
    "### Importer les packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09Egu-FpMTD0",
    "outputId": "a66bf792-d5a9-42fd-e7b0-5279c480271f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# importing tokenizers\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# importing pos taggers\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# importing the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# importing Porter and Lancaster stemmers \n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "# importing WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# importing wordnet\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"POS Tagging is a process to mark up the words in text format for a particular part of a speech, based on its definition and context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Tokenization - Punctuations are keeped with word_tokenizer\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POS', 'Tagging', 'is', 'a', 'process', 'to', 'mark', 'up', 'the', 'words', 'in', 'text', 'format', 'for', 'a', 'particular', 'part', 'of', 'a', 'speech', ',', 'based', 'on', 'its', 'definition', 'and', 'context', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuations are removed with RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POS', 'Tagging', 'is', 'a', 'process', 'to', 'mark', 'up', 'the', 'words', 'in', 'text', 'format', 'for', 'a', 'particular', 'part', 'of', 'a', 'speech', 'based', 'on', 'its', 'definition', 'and', 'context']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sw = stopwords.words('english')\n",
    "\n",
    "en_sw[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_sw = stopwords.words('arabic')\n",
    "\n",
    "ar_sw[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stopwords from tokens\n",
    "\n",
    "clean_tokens = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in en_sw:\n",
    "        clean_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POS', 'Tagging', 'process', 'mark', 'words', 'text', 'format', 'particular', 'part', 'speech', ',', 'based', 'definition', 'context', '.']\n"
     ]
    }
   ],
   "source": [
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "# Autres tokenizers : Tweet Tokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tokens = tknzr.tokenize(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a text written.', 'It uses U.S. english to illustrate sentence tokenization.']\n"
     ]
    }
   ],
   "source": [
    "# Autres tokenizers : Sentence Tokenizer\n",
    "sentences = 'This is a text written. It uses U.S. english to illustrate sentence tokenization.'\n",
    "\n",
    "sent_tokens = sent_tokenize(sentences)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging is a process to mark up the words in text format for a particular part of a speech based on its definition and context. It categorizes the tokens in a text as nouns, verbs, adjectives, and so on.\n",
    "\n",
    "NLTK : nltk.tag.pos_tag(tokens_list, tagset=None, lang='eng') : list(tuple(str, str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('POS', 'NNP'), ('Tagging', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('process', 'NN'), ('to', 'TO'), ('mark', 'VB'), ('up', 'RP'), ('the', 'DT'), ('words', 'NNS'), ('in', 'IN'), ('text', 'JJ'), ('format', 'NN'), ('for', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('part', 'NN'), ('of', 'IN'), ('a', 'DT'), ('speech', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('its', 'PRP$'), ('definition', 'NN'), ('and', 'CC'), ('context', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Default tagset : PennTreebank tagset\n",
    "tags = pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('POS', 'NOUN'), ('Tagging', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('process', 'NOUN'), ('to', 'PRT'), ('mark', 'VERB'), ('up', 'PRT'), ('the', 'DET'), ('words', 'NOUN'), ('in', 'ADP'), ('text', 'ADJ'), ('format', 'NOUN'), ('for', 'ADP'), ('a', 'DET'), ('particular', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('speech', 'NOUN'), ('based', 'VERB'), ('on', 'ADP'), ('its', 'PRON'), ('definition', 'NOUN'), ('and', 'CONJ'), ('context', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# With Universal dependencies tagset\n",
    "tags = pos_tag(tokens, tagset = \"universal\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'PRT', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'ADP', 'PRON', 'NOUN', 'CONJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "# Get all tags\n",
    "tags_list = []\n",
    "for tag in tags:\n",
    "    tags_list.append(tag[1])\n",
    "print(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 9, 'DET': 4, 'ADP': 4, 'VERB': 3, 'PRT': 2, 'ADJ': 2, 'PRON': 1, 'CONJ': 1})\n"
     ]
    }
   ],
   "source": [
    "# Count the number of each tag\n",
    "counts = Counter(tag for tag in tags_list)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 9, 'DET': 4, 'ADP': 4, 'VERB': 3, 'PRT': 2, 'ADJ': 2, 'PRON': 1, 'CONJ': 1})\n"
     ]
    }
   ],
   "source": [
    "# Count the number of each tag\n",
    "counts = Counter(tag for token, tag in tags)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGOdsCIqp4K9"
   },
   "source": [
    "### **Stemming** - Porter and Lancaster stemmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster Stemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Aggressive stemming. Over-stemming causes the stems to be not linguistic, or they may have no meaning. Lancaster produces an even shorter stem than Porter because of iterations and over-stemming is occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVmHgw4GMb7G",
    "outputId": "e44459cf-c764-40f4-9b8d-c05a162ec29b"
   },
   "outputs": [],
   "source": [
    "# Create an object of class PorterStemmer and LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabl\n"
     ]
    }
   ],
   "source": [
    "porter_stem = porter.stem(\"probably\")\n",
    "print(porter_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob\n"
     ]
    }
   ],
   "source": [
    "lancaster_stem = lancaster.stem(\"probably\")\n",
    "print(lancaster_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer examples: \n",
      "chang\n",
      "troubl\n",
      "troubl\n",
      "cat\n",
      "character\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter Stemmer examples: \")\n",
    "print(porter.stem(\"changes\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"characterization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer examples : \n",
      "chang\n",
      "troubl\n",
      "troubl\n",
      "cat\n",
      "charact\n"
     ]
    }
   ],
   "source": [
    "print(\"Lancaster Stemmer examples : \")\n",
    "print(lancaster.stem(\"changes\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"characterization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64YHov0jMkST",
    "outputId": "654dabb0-2353-4bd1-fa82-7a81b6982919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend               friend               friend\n",
      "friendship           friendship           friend\n",
      "friends              friend               friend\n",
      "friendships          friendship           friend\n",
      "stabil               stabil               stabl\n",
      "destabilize          destabil             dest\n",
      "misunderstanding     misunderstand        misunderstand\n",
      "railroad             railroad             railroad\n",
      "moonlight            moonlight            moonlight\n",
      "football             footbal              footbal\n"
     ]
    }
   ],
   "source": [
    "# Stemming a list of words\n",
    "\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\", \"destabilize\", \"misunderstanding\", \"railroad\", \"moonlight\", \"football\"]\n",
    "\n",
    "for word in word_list:\n",
    "    print(f\"{word:20} {porter.stem(word):20} {lancaster.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StUuWlPgMwyE",
    "outputId": "0efa1ce9-b552-4d7d-d858-98a8c1bdb7b3"
   },
   "outputs": [],
   "source": [
    "# Stemming a sentence using a defined function : stem_sentence\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # Stemming\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(porter.stem(token))\n",
    "    \n",
    "    return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed sentence:  python are veri intellig , and work veri pythonli and now they are python their way to success .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "stems = stem_sentence(sentence)\n",
    "\n",
    "print('Stemmed sentence: ', stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUaLlSPoq_2W"
   },
   "source": [
    "### **Lemmatization**  - WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQQYLwtlRaEj",
    "outputId": "6cdceafb-3056-44f5-f8fc-935ce51a8e58"
   },
   "outputs": [],
   "source": [
    "# Instantiating the lemmaztizer object\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "foot\n",
      "are\n",
      "change\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize a single word without context\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "print(lemmatizer.lemmatize(\"changes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "swim\n",
      "swimming\n",
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize a single word with context\n",
    "print(lemmatizer.lemmatize(\"are\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"swimming\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"swimming\", pos='n'))\n",
    "print(lemmatizer.lemmatize(\"stripes\", pos='v')) \n",
    "print(lemmatizer.lemmatize(\"stripes\", pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YILtwedsr0uu",
    "outputId": "a305d756-deb0-40c5-caf5-3b184522b92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                   He\n",
      "is                   is\n",
      "running              running\n",
      "and                  and\n",
      "eating               eating\n",
      "at                   at\n",
      "same                 same\n",
      "time                 time\n",
      "He                   He\n",
      "has                  ha\n",
      "bad                  bad\n",
      "habit                habit\n",
      "of                   of\n",
      "swimming             swimming\n",
      "after                after\n",
      "playing              playing\n",
      "long                 long\n",
      "hours                hour\n",
      "in                   in\n",
      "the                  the\n",
      "Sun                  Sun\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize a sentence\n",
    "sentence = \"He is running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# Tokenize the sentence into a list of words without punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Lemmatize without context\n",
    "for token in tokens:\n",
    "    print(f\"{token:20} {lemmatizer.lemmatize(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming Vs. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leav\n",
      "leaf\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"leaves\"))\n",
    "print(porter.stem(\"leafs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave\n",
      "leaf\n",
      "leaf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"leaves\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"leaves\", pos='n'))\n",
    "print(lemmatizer.lemmatize(\"leafs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams, Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **N-gram** is a contiguous sequence of n items from a given sample of text or speech. In Natural Language Processing, the concept of N-gram is widely used for text analysis. An N-gram of size 1 is referred to as a “unigram“, size 2 is a “bigram”, size 3 is a “trigram”.\n",
    "\n",
    "- **Bigrams** combination of two words\n",
    "- **Trigrams** combination of three words\n",
    "- **N-grams** combination of n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"POS Tagging is a process to mark up the words in text format for a particular part of a speech, based on its definition and context.\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POS', 'Tagging', 'is', 'a', 'process', 'to', 'mark', 'up', 'the', 'words', 'in', 'text', 'format', 'for', 'a', 'particular', 'part', 'of', 'a', 'speech', ',', 'based', 'on', 'its', 'definition', 'and', 'context', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('POS', 'Tagging'), ('Tagging', 'is'), ('is', 'a'), ('a', 'process'), ('process', 'to'), ('to', 'mark'), ('mark', 'up'), ('up', 'the'), ('the', 'words'), ('words', 'in'), ('in', 'text'), ('text', 'format'), ('format', 'for'), ('for', 'a'), ('a', 'particular'), ('particular', 'part'), ('part', 'of'), ('of', 'a'), ('a', 'speech'), ('speech', ','), (',', 'based'), ('based', 'on'), ('on', 'its'), ('its', 'definition'), ('definition', 'and'), ('and', 'context'), ('context', '.')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(tokens))\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('POS', 'Tagging', 'is'), ('Tagging', 'is', 'a'), ('is', 'a', 'process'), ('a', 'process', 'to'), ('process', 'to', 'mark'), ('to', 'mark', 'up'), ('mark', 'up', 'the'), ('up', 'the', 'words'), ('the', 'words', 'in'), ('words', 'in', 'text'), ('in', 'text', 'format'), ('text', 'format', 'for'), ('format', 'for', 'a'), ('for', 'a', 'particular'), ('a', 'particular', 'part'), ('particular', 'part', 'of'), ('part', 'of', 'a'), ('of', 'a', 'speech'), ('a', 'speech', ','), ('speech', ',', 'based'), (',', 'based', 'on'), ('based', 'on', 'its'), ('on', 'its', 'definition'), ('its', 'definition', 'and'), ('definition', 'and', 'context'), ('and', 'context', '.')]\n"
     ]
    }
   ],
   "source": [
    "trigrams = list(nltk.trigrams(tokens))\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet - Distance and Word similarity - Wu-Palmer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wordnet.synsets('dog')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = wordnet.synsets('cat')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = wordnet.synsets('play')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.wup_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.wup_similarity(play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet - Synonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small\n",
      "small\n",
      "small\n",
      "little\n",
      "minor\n",
      "modest\n",
      "small\n",
      "small-scale\n",
      "pocket-size\n",
      "pocket-sized\n",
      "little\n",
      "small\n",
      "small\n",
      "humble\n",
      "low\n",
      "lowly\n",
      "modest\n",
      "small\n",
      "little\n",
      "minuscule\n",
      "small\n",
      "little\n",
      "small\n",
      "small\n",
      "modest\n",
      "small\n",
      "belittled\n",
      "diminished\n",
      "small\n",
      "small\n"
     ]
    }
   ],
   "source": [
    "for ss in wordnet.synsets('small'):\n",
    "    for name in ss.lemma_names():\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ecrire une fonction get_sent_stems(str) qui prend comme paramètre une phrase et qui retourne la liste des stems de chaque mot de cette phrase, sans les ponctuations et sans les stopwords. \n",
    "\n",
    "- Tester cette fonction avec une phrase de votre choix, puis convertir la liste des stems retournée en une chaine de caractères str. Afficher cette chaine.\n",
    "\n",
    "- Soit la chaine de caractères suivante : \"This is a text written. It uses U.S. english to illustrate sentence tokenization and so on.”. Ecrire le code permettant d’afficher les informations suivantes pour chaque mot de la chaine :\n",
    "    Token\t---  Stem  ----\tLemma  ---\tPOS Universal ---- POS PennTreeBank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_stems(sentence):\n",
    "    \n",
    "    # Tokenization, without punkt\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Clean stopwods\n",
    "    clean_tokens = []\n",
    "    sw = stopwords.words('english')\n",
    "    for token in tokens:\n",
    "        if token not in sw:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    # Stemming\n",
    "    stems = []\n",
    "    porter = PorterStemmer()\n",
    "    for token in clean_tokens:\n",
    "        stems.append(porter.stem(token))\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'intellig', 'work', 'pythonli', 'python', 'way', 'success']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
    "stems = get_sent_stems(sentence)\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python intellig work pythonli python way success\n"
     ]
    }
   ],
   "source": [
    "mystr = \" \".join(stems)\n",
    "print(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a text written. It uses U.S. english to illustrate sentence tokenization and so on.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This                 thi                  This                 DET                  DT\n",
      "is                   is                   is                   VERB                 VBZ\n",
      "a                    a                    a                    DET                  DT\n",
      "text                 text                 text                 NOUN                 NN\n",
      "written              written              written              VERB                 VBN\n",
      ".                    .                    .                    .                    .\n",
      "It                   it                   It                   PRON                 PRP\n",
      "uses                 use                  us                   NOUN                 NNS\n",
      "U.S.                 u.s.                 U.S.                 NOUN                 NNP\n",
      "english              english              english              ADJ                  JJ\n",
      "to                   to                   to                   PRT                  TO\n",
      "illustrate           illustr              illustrate           NOUN                 NN\n",
      "sentence             sentenc              sentence             NOUN                 NN\n",
      "tokenization         token                tokenization         NOUN                 NN\n",
      "and                  and                  and                  CONJ                 CC\n",
      "so                   so                   so                   ADV                  RB\n",
      "on                   on                   on                   ADP                  IN\n",
      ".                    .                    .                    .                    .\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "for token in tokens:\n",
    "    porter = PorterStemmer()\n",
    "    stem = porter.stem(token)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = lemmatizer.lemmatize(token)\n",
    "    \n",
    "    postag = pos_tag([token])\n",
    "    postag_uni = pos_tag([token], tagset='universal')\n",
    "    \n",
    "    print(f\"{token:20} {stem:20} {lemma:20} {postag_uni[0][1]:20} {postag[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Soundex Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['aehiouwy', #0\n",
    "    'bfpv', #1\n",
    "    'cgjkqsxz', #2\n",
    "    'dt', #3\n",
    "    'l', #4\n",
    "    'mn', #5\n",
    "    'r'] #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = {'a': '0',  'c': '2', 'b': '1', 'e': '0', 'd': '3', 'g': '2', 'f': '1', 'i': '0', 'h': '0', 'k': '2', \n",
    "            'j': '2', 'm': '5', 'l': '4', 'o': '0', 'n': '5', 'q': '2', 'p': '1', 's': '2', 'r': '6', 'u': '0', \n",
    "            't': '3', 'w': '0', 'v': '1', 'y': '0', 'x': '2', 'z': '2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex(word):\n",
    "    \"\"\" \n",
    "      Soundex module conforming to Knuth's algorithm implementation 2000-12-24 by Gregory Jorgensen public domain\n",
    "    \"\"\"\n",
    "    \n",
    "    sndx = ''\n",
    "    \n",
    "    firstchar = word[0].upper()\n",
    "    word = word[1:]\n",
    "\n",
    "    for car in word.lower():\n",
    "        digit = numerics[car]\n",
    "       \n",
    "        if not sndx or digit != sndx[-1]:\n",
    "            sndx += digit\n",
    "\n",
    "    sndx = sndx.replace('0','')\n",
    "    \n",
    "    sndx = firstchar + sndx\n",
    "\n",
    "    sndx =  sndx + '0000'\n",
    "    \n",
    "    return sndx[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physique P220\n",
      "physik P220\n",
      "phosphore P216\n",
      "fosfor F216\n",
      "hello H400\n",
      "robert R163\n",
      "rupert R163\n"
     ]
    }
   ],
   "source": [
    "words =[ \"physique\", \"physik\", \"phosphore\", \"fosfor\", \"hello\", \"robert\", \"rupert\"]\n",
    "\n",
    "for word in words :\n",
    "    code = soundex(word)\n",
    "    print(word, code)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Série_TP_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
