{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SÃ©rie_TP_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8vQKUh-pjdJ"
      },
      "source": [
        "## **Analyse Lexicale - PreProcessing - Stemming et Lemmatisation avec NLTK.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5JTPS0qrPZZ"
      },
      "source": [
        "### **Importer les librairies  modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Egu-FpMTD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66bf792-d5a9-42fd-e7b0-5279c480271f"
      },
      "source": [
        "# How to install and import nltk\n",
        "# !pip install nltk\n",
        "import nltk\n",
        "\n",
        "# importing tokenization \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# importing the stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# importing Porter and Lancaster stemmers from nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "# importing WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGOdsCIqp4K9"
      },
      "source": [
        "### **Stemming** - Porter and Lancaster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVmHgw4GMb7G",
        "outputId": "e44459cf-c764-40f4-9b8d-c05a162ec29b"
      },
      "source": [
        "# LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Aggressive stemming.\n",
        "# Over-stemming causes the stems to be not linguistic, or they may have no meaning. \n",
        "# Lancaster produces an even shorter stem than Porter because of iterations and over-stemming is occurred.\n",
        "\n",
        "# create an object of class PorterStemmer and LancasterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# - Stemming a word - PorterStemmer\n",
        "print(\"- Porter Stemmer\")\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"trouble\"))\n",
        "print(porter.stem(\"troubling\"))\n",
        "print(porter.stem(\"troubled\"))\n",
        "print(porter.stem(\"probably\"))\n",
        "\n",
        "# - Stemming a word - LancasterStemmer \n",
        "print(\"- Lancaster Stemmer\")\n",
        "print(lancaster.stem(\"cats\"))\n",
        "print(lancaster.stem(\"trouble\"))\n",
        "print(lancaster.stem(\"troubling\"))\n",
        "print(lancaster.stem(\"troubled\"))\n",
        "print(lancaster.stem(\"probably\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Porter Stemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "probabl\n",
            "- Lancaster Stemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "prob\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64YHov0jMkST",
        "outputId": "654dabb0-2353-4bd1-fa82-7a81b6982919"
      },
      "source": [
        "# - Stemming a list of words\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
        "print(\"{0:20}{1:20}{2}\".format(\"Word\", \"Porter Stemmer\", \"lancaster Stemmer\"))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2}\".format(word, porter.stem(word), lancaster.stem(word)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Porter Stemmer      lancaster Stemmer\n",
            "friend              friend              friend\n",
            "friendship          friendship          friend\n",
            "friends             friend              friend\n",
            "friendships         friendship          friend\n",
            "stabil              stabil              stabl\n",
            "destabilize         destabil            dest\n",
            "misunderstanding    misunderstand       misunderstand\n",
            "railroad            railroad            railroad\n",
            "moonlight           moonlight           moonlight\n",
            "football            footbal             footbal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6gIW4EB1MrLK",
        "outputId": "f0886721-ac97-44d6-950e-d1cddd173b89"
      },
      "source": [
        "# - Stemming a sentence without tokenization\n",
        "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
        "porter.stem(sentence)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u6ctjL8tn4CS",
        "outputId": "2936e642-5c45-47f9-c1d3-e4332fac7879"
      },
      "source": [
        "lancaster.stem(sentence)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StUuWlPgMwyE",
        "outputId": "0efa1ce9-b552-4d7d-d858-98a8c1bdb7b3"
      },
      "source": [
        "# - Stemming a sentence with word tokenization (punctuations are keeped with word_tokenizer)\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
        "\n",
        "def stemSentence(sentence):\n",
        "\t  # Tokenization\n",
        "    token_words = word_tokenize(sentence)\n",
        "    print('Tokens:', token_words)\n",
        "    # Stemming\n",
        "    stem_sentence = []\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "    return \" \".join(stem_sentence)\n",
        "    \n",
        "stems = stemSentence(sentence)\n",
        "print('Stems: ', stems)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Tokens: ['Pythoners', 'are', 'very', 'intelligent', ',', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
            "Stems:  python are veri intellig , and work veri pythonli and now they are python their way to success .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-brQIhacNnJS",
        "outputId": "8c69c170-42e7-46e4-8b58-c2ee74c21543"
      },
      "source": [
        "# - Stemming a sentence with tokenization, without stopwords\n",
        "\n",
        "# downloading stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# assigning the english stop-words to the sw list\n",
        "sw = stopwords.words('english')\n",
        "\n",
        "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
        "\n",
        "def cleanStemSentence(sentence):\n",
        "    token_words = word_tokenize(sentence)\n",
        "    print('Tokens - before:', token_words)\n",
        "    # eliminate the stop words from the tokens\n",
        "    clean_tokens = [token for token in token_words if token not in sw]\n",
        "    print('Tokens - after:', clean_tokens)\n",
        "    stem_sentence = []\n",
        "    for word in clean_tokens:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "    return \" \".join(stem_sentence)\n",
        "\n",
        "stems = cleanStemSentence(sentence)\n",
        "print('Stems: ', stems)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Tokens - before: ['Pythoners', 'are', 'very', 'intelligent', ',', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
            "Tokens - after: ['Pythoners', 'intelligent', ',', 'work', 'pythonly', 'pythoning', 'way', 'success', '.']\n",
            "Stems:  python intellig , work pythonli python way success .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elytVWh2X9nv",
        "outputId": "44ffda97-64e0-4c48-9fde-fe454c19760c"
      },
      "source": [
        "# - Stemming a sentence with word tokenization (remove punctuations with RegexpTokenizer)\n",
        "\n",
        "sentence = \"Pythoners are very intelligent, and work very pythonly and now they are pythoning their way to success.\"\n",
        "\n",
        "def stemSentence(sentence):\n",
        "\t  # Tokenization\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    token_words = tokenizer.tokenize(sentence)\n",
        "    print('Tokens:', token_words)\n",
        "    # Stemming\n",
        "    stem_sentence = []\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "    return \" \".join(stem_sentence)\n",
        "    \n",
        "stems = stemSentence(sentence)\n",
        "print('Stems: ', stems)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens: ['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success']\n",
            "Stems:  python are veri intellig and work veri pythonli and now they are python their way to success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oahl9nK4uE3H",
        "outputId": "fd7b2a95-06f9-4a6b-f0bd-5490cb22b362"
      },
      "source": [
        "# - Stemming a document\n",
        "\n",
        "# with open(\"my_text.txt\") as file:\n",
        "# \tmy_lines_list = file.readlines()\n",
        "\n",
        "my_lines_list = ['He was running and eating at same time.', 'He has bad habit of swimming after playing long hours in the Sun.']\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words = word_tokenize(sentence)\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "    return \" \".join(stem_sentence)\n",
        "\n",
        "# Stemming the first line of the document/list\n",
        "stems = stemSentence(my_lines_list[0])\n",
        "print('Sentence: ', my_lines_list[0])\n",
        "print('Stemmed sentence: ', stems)\n",
        "\n",
        "# Stemming all lines of the document/list\n",
        "for line in my_lines_list:\n",
        "    stem_sentence = stemSentence(line)\n",
        "    print('Sentence: ', line)\n",
        "    print('Stemmed sentence: ', stem_sentence)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  He was running and eating at same time.\n",
            "Stemmed sentence:  He wa run and eat at same time .\n",
            "Sentence:  He was running and eating at same time.\n",
            "Stemmed sentence:  He wa run and eat at same time .\n",
            "Sentence:  He has bad habit of swimming after playing long hours in the Sun.\n",
            "Stemmed sentence:  He ha bad habit of swim after play long hour in the sun .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUaLlSPoq_2W"
      },
      "source": [
        "### **Lemmatization**  - WordNet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQQYLwtlRaEj",
        "outputId": "6cdceafb-3056-44f5-f8fc-935ce51a8e58"
      },
      "source": [
        "# - Lemmatization - WordNet Lemmatizer without context\n",
        "\n",
        "# download wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# instantiating the lemmaztizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize a single word without context\n",
        "print(lemmatizer.lemmatize(\"bats\"))\n",
        "print(lemmatizer.lemmatize(\"feet\"))\n",
        "print(lemmatizer.lemmatize(\"are\"))\n",
        "\n",
        "# Lemmatize a single word with context\n",
        "print(lemmatizer.lemmatize(\"are\", pos='v'))\n",
        "print(lemmatizer.lemmatize(\"swimming\", pos='v'))\n",
        "print(lemmatizer.lemmatize(\"swimming\", pos='n'))\n",
        "print(lemmatizer.lemmatize(\"stripes\", pos='v')) \n",
        "print(lemmatizer.lemmatize(\"stripes\", pos='n'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "bat\n",
            "foot\n",
            "are\n",
            "be\n",
            "swim\n",
            "swimming\n",
            "strip\n",
            "stripe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YILtwedsr0uu",
        "outputId": "a305d756-deb0-40c5-caf5-3b184522b92c"
      },
      "source": [
        "# - Lemmatize a sentence\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "\n",
        "# tokenize the sentence into a list of words without punctuations\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "sentence_words = tokenizer.tokenize(sentence)\n",
        "\n",
        "# without context\n",
        "print(\"{0:20}{1}\".format(\"Word\", \"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1}\".format(word, lemmatizer.lemmatize(word)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Lemma\n",
            "He                  He\n",
            "was                 wa\n",
            "running             running\n",
            "and                 and\n",
            "eating              eating\n",
            "at                  at\n",
            "same                same\n",
            "time                time\n",
            "He                  He\n",
            "has                 ha\n",
            "bad                 bad\n",
            "habit               habit\n",
            "of                  of\n",
            "swimming            swimming\n",
            "after               after\n",
            "playing             playing\n",
            "long                long\n",
            "hours               hour\n",
            "in                  in\n",
            "the                 the\n",
            "Sun                 Sun\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}